{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import random\n",
        "import time\n",
        "\n",
        "def get_valid_ai_choice(model, max_retries=3):\n",
        "    options = [\"石頭\", \"布\", \"剪刀\"]\n",
        "    prompt = \"請選擇 石頭、布 或 剪刀。請只回答這三個詞中的一個，其他內容不要輸出。\"\n",
        "\n",
        "    for _ in range(max_retries):\n",
        "        time.sleep(1)\n",
        "        try:\n",
        "            response = model(prompt, max_length=5, do_sample=True)[0]['generated_text'].strip()\n",
        "            response = response.replace(\"。\", \"\").replace(\",\", \"\").replace(\"\\n\", \"\").strip()\n",
        "\n",
        "            if response in options:\n",
        "                return response\n",
        "            else:\n",
        "                print(f\"AI 選擇了無效的選項：{response}，請再試一次。\")\n",
        "        except Exception:\n",
        "            print(\"AI 發生錯誤，電腦隨機選擇一個選項。\")\n",
        "            return random.choice(options)\n",
        "\n",
        "    print(\"超過重試次數，隨機選擇一個選項。\")\n",
        "    return random.choice(options)\n",
        "\n",
        "def determine_winner(choice1, choice2):\n",
        "    if choice1 == choice2:\n",
        "        return 0  # 平局\n",
        "    elif (choice1 == \"石頭\" and choice2 == \"剪刀\") or \\\n",
        "         (choice1 == \"布\" and choice2 == \"石頭\") or \\\n",
        "         (choice1 == \"剪刀\" and choice2 == \"布\"):\n",
        "        return 1  # AI 1 贏了\n",
        "    else:\n",
        "        return 2  # AI 2 贏了\n",
        "\n",
        "def ai_chat(model, round_number, ai1_choice, ai2_choice, winner):\n",
        "    prompt = (\n",
        "        f\"第 {round_number} 輪：AI 1 選擇 {ai1_choice}，AI 2 選擇 {ai2_choice}。\\n\"\n",
        "        f\"結果：{'平局' if winner == 0 else 'AI 1 贏了' if winner == 1 else 'AI 2 贏了'}。\\n\"\n",
        "        \"請模擬 AI 1 和 AI 2 之間的對話，討論比賽結果。\"\n",
        "    )\n",
        "    response = model(prompt, max_length=50, do_sample=True)[0]['generated_text']\n",
        "    return response.strip()\n",
        "\n",
        "def ai_vs_ai_rock_paper_scissors():\n",
        "    model = pipeline(\"text-generation\", model=\"google-t5/t5-small\")\n",
        "    ai1_wins = 0\n",
        "    ai2_wins = 0\n",
        "    rounds = 1\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"輸入 EXIT 退出遊戲，按 ENTER 繼續：\").strip().upper()\n",
        "        if user_input == \"EXIT\":\n",
        "            break\n",
        "\n",
        "        print(f\"\\n第 {rounds} 輪：\")\n",
        "\n",
        "        ai1_choice = get_valid_ai_choice(model)\n",
        "        print(f\"AI 1 選擇：{ai1_choice}\")\n",
        "\n",
        "        ai2_choice = get_valid_ai_choice(model)\n",
        "        print(f\"AI 2 選擇：{ai2_choice}\")\n",
        "\n",
        "        result = determine_winner(ai1_choice, ai2_choice)\n",
        "        if result == 1:\n",
        "            print(\"結果：AI 1 贏了！\")\n",
        "            ai1_wins += 1\n",
        "        elif result == 2:\n",
        "            print(\"結果：AI 2 贏了！\")\n",
        "            ai2_wins += 1\n",
        "        else:\n",
        "            print(\"結果：平局！\")\n",
        "\n",
        "        print(f\"AI 對話：\\n{ai_chat(model, rounds, ai1_choice, ai2_choice, result)}\\n\")\n",
        "        rounds += 1\n",
        "\n",
        "    print(\"\\n遊戲結束！\")\n",
        "    print(f\"AI 1 勝利次數：{ai1_wins}, AI 2 勝利次數：{ai2_wins}\")\n",
        "\n",
        "# 開始遊戲\n",
        "ai_vs_ai_rock_paper_scissors()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyXLEkcOQrUE",
        "outputId": "3fbac697-8520-4ef8-a643-243753538a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "輸入 EXIT 退出遊戲，按 ENTER 繼續：\n",
            "\n",
            "第 1 輪：\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI 選擇了無效的選項：請選擇 石頭、布 或 剪刀請只回答這三個詞中的一個，其他內容不要輸出，請再試一次。\n",
            "AI 選擇了無效的選項：請選擇 石頭、布 或 剪刀請只回答這三個詞中的一個，其他內容不要輸出，請再試一次。\n",
            "AI 選擇了無效的選項：請選擇 石頭、布 或 剪刀請只回答這三個詞中的一個，其他內容不要輸出，請再試一次。\n",
            "超過重試次數，隨機選擇一個選項。\n",
            "AI 1 選擇：石頭\n",
            "AI 選擇了無效的選項：請選擇 石頭、布 或 剪刀請只回答這三個詞中的一個，其他內容不要輸出，請再試一次。\n",
            "AI 選擇了無效的選項：請選擇 石頭、布 或 剪刀請只回答這三個詞中的一個，其他內容不要輸出，請再試一次。\n",
            "AI 選擇了無效的選項：請選擇 石頭、布 或 剪刀請只回答這三個詞中的一個，其他內容不要輸出，請再試一次。\n",
            "超過重試次數，隨機選擇一個選項。\n",
            "AI 2 選擇：石頭\n",
            "結果：平局！\n",
            "AI 對話：\n",
            "第 1 輪：AI 1 選擇 石頭，AI 2 選擇 石頭。\n",
            "結果：平局。\n",
            "請模擬 AI 1 和 AI 2 之間的對話，討論比賽結果。 AI 1  AI 2\n",
            "\n"
          ]
        }
      ]
    }
  ]
}